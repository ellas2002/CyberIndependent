Reinforcement Learning (Richard S. Sutton; Andrew G. Barto)
  Introduction
    Direct sensorimotor connection: direct pathway from the sensory to motor cortex: allows sensory input to influence motor output
	  Looked at as the way we learn information about environment and self

    1.1 Reinforcement Learning
      Machine learning paradigms
        Supervised
        unsupervised 
          reinforcement

Reinforcement Learning: learning what to do (map situations to actions) to maximize a numerical reward signal
	Learner has to discover what action yields the most value by trying them out
Two most important features for this type of learning
Trial and error search
Delayed reward	
Idea that an immediate action can affect the next situation…etc.
Unsupervised vs supervised learning
Supervised: learn from training set of guiding examples given by a supervisor
Not good in interactive environment
Got to learn from experience 
Unsupervised can be about fading structures hidden a lot of unlabeled data
Challenge for reinforcement learning:
Will have to make mistakes or fail 
Have to balance exploration and exploitation
Exploration
Has to choose to do things it hasn't before to find better option
Exploitation
Choosing to do things that worked out in the past
Key feature of reinforcement learning
Considers the whole problem 
Other paradigms consider goals
Which is a limitation
Agents have 
Specific goals
Senses aspects of the env
Can choose action that affect env
Starts out with no info about env
Has to make decisions about now and future
1.3 Elements of Reinforcement Learning
Main sub-elements of a reinforcement learning system
Policy
Agents way of behaving a given time
May be stochastic
Reward signal
What's good immediately 
Each time step, env sends a reward (single number) to agent
Agents wants to maximize that reward
May be stochastic
primary
Value function
What's good in the long wrong
Will be estimated and re-estimated based on observations from their lifetime
secondary
Model
Allows for inferences about how an env behaves
Used for planning 
1.4 Limitations and Scope
Relies heavily on state
State is whatever info is available to agent about env
Book is looking deciding what action to take as a function of whatever state signal is available 
Part 1: Tabular Solutions Methods
State and actions spaces are small
Can be represented as arrays or tables
Methods can then find the optimal value function and policy
Chapter 2: Multi-armed bandits
Important difference about reinforcement
Evaluates the action rather than by instruction
Evaluative feedback cs instructive
Evaluate indicates how good the action taken was
Action taken
Instructive indicates best or worst action taken
Independent of action
2.1 A k-armed Bandit Problem
Idea
Faced with a choice with k different options/action
Each choice gives you a numerical reward
Which is chosen from a stationary probability distribution that depends on action selected 
Goal is to maximize expected total reward
E.g. slot machine
K action has a expected or mean reward when an action is selected
 q⇤(a) . = E[Rt |At=a]. 			At = value of action selected on timestamp t
					Rt = corresponding reward
					A = arbitrary action
					q⇤(a) = expected reward given taht a is slected
